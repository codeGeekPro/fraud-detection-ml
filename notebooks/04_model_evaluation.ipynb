{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2da51aa",
   "metadata": {},
   "source": [
    "# Ã‰valuation des ModÃ¨les de DÃ©tection de Fraudes\n",
    "\n",
    "Ce notebook compare et Ã©value les performances des diffÃ©rents modÃ¨les avec des analyses approfondies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096b818",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Charger les modÃ¨les entraÃ®nÃ©s et les comparer\n",
    "- Analyser les performances dÃ©taillÃ©es par mÃ©triques\n",
    "- Ã‰tudier les erreurs et les cas limites\n",
    "- Ã‰valuer la robustesse des modÃ¨les\n",
    "- GÃ©nÃ©rer des rapports de performance complets\n",
    "- Identifier les axes d'amÃ©lioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ajout du chemin racine au sys.path\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Imports de base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Imports ML et mÃ©triques\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    average_precision_score, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Imports locaux\n",
    "from src.utils.metrics import calculate_metrics, pr_auc_score\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… Environnement configurÃ© avec succÃ¨s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854ea76",
   "metadata": {},
   "source": [
    "## 1. Chargement des ModÃ¨les et DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modÃ¨le entraÃ®nÃ©\n",
    "print(\"ðŸ”„ Chargement du modÃ¨le et des donnÃ©es...\")\n",
    "\n",
    "# Chargement du modÃ¨le\n",
    "model_path = \"../models/trained/best_model.pkl\"\n",
    "if os.path.exists(model_path):\n",
    "    best_model = joblib.load(model_path)\n",
    "    print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s\")\n",
    "else:\n",
    "    print(\"âŒ ModÃ¨le non trouvÃ©. Veuillez exÃ©cuter le notebook d'entraÃ®nement d'abord.\")\n",
    "    raise FileNotFoundError(\"ModÃ¨le non trouvÃ©\")\n",
    "\n",
    "# Chargement des mÃ©tadonnÃ©es\n",
    "metadata_path = \"../models/metadata/model_metadata.json\"\n",
    "if os.path.exists(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"âœ… MÃ©tadonnÃ©es chargÃ©es\")\n",
    "else:\n",
    "    print(\"âš ï¸ MÃ©tadonnÃ©es non trouvÃ©es\")\n",
    "    metadata = {}\n",
    "\n",
    "# Chargement des donnÃ©es de test\n",
    "features_path = \"../data/processed/features_engineered.csv\"\n",
    "if os.path.exists(features_path):\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(\"âœ… DonnÃ©es de test chargÃ©es\")\n",
    "else:\n",
    "    print(\"âŒ DonnÃ©es de test non trouvÃ©es\")\n",
    "    raise FileNotFoundError(\"DonnÃ©es de test non trouvÃ©es\")\n",
    "\n",
    "print(f\"ðŸ“Š DonnÃ©es : {df.shape}\")\n",
    "print(f\"ðŸ† ModÃ¨le chargÃ© : {metadata.get('model_name', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aad8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©paration des donnÃ©es de test\n",
    "print(\"ðŸ”„ PrÃ©paration des donnÃ©es de test...\")\n",
    "\n",
    "# SÃ©paration features/cible\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# CrÃ©ation d'un jeu de test reprÃ©sentatif\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, X_test, _, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Test set : {X_test.shape}\")\n",
    "print(f\"ðŸ“ˆ Distribution classes test : {y_test.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# PrÃ©dictions du modÃ¨le\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ… PrÃ©dictions gÃ©nÃ©rÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21a309",
   "metadata": {},
   "source": [
    "## 2. Analyse des MÃ©triques Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94751cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des mÃ©triques dÃ©taillÃ©es\n",
    "print(\"ðŸ“Š Analyse des mÃ©triques principales...\")\n",
    "\n",
    "# MÃ©triques de base\n",
    "metrics = calculate_metrics(y_test, y_pred, y_proba.reshape(-1, 1))\n",
    "\n",
    "# MÃ©triques supplÃ©mentaires\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "additional_metrics = {\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "    'cohen_kappa': cohen_kappa_score(y_test, y_pred),\n",
    "    'brier_score': brier_score_loss(y_test, y_proba)\n",
    "}\n",
    "\n",
    "# Combinaison des mÃ©triques\n",
    "all_metrics = {**metrics, **additional_metrics}\n",
    "\n",
    "print(\"ðŸ“ˆ MÃ‰TRIQUES D'Ã‰VALUATION COMPLÃˆTES :\")\n",
    "print(\"-\" * 50)\n",
    "for metric_name, value in all_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(\"25s\")\n",
    "    else:\n",
    "        print(\"25s\")\n",
    "\n",
    "print(\"\\nðŸ“‹ RAPPORT DE CLASSIFICATION DÃ‰TAILLÃ‰ :\")\n",
    "print(classification_report(y_test, y_pred, target_names=['LÃ©gitime', 'Frauduleuse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des mÃ©triques principales\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('MÃ©triques d\\'Ã‰valuation - Analyse DÃ©taillÃ©e', fontsize=16)\n",
    "\n",
    "# MÃ©triques Ã  visualiser\n",
    "viz_metrics = ['precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'balanced_accuracy']\n",
    "viz_values = [all_metrics[m] for m in viz_metrics]\n",
    "\n",
    "# Bar plot des mÃ©triques\n",
    "bars = axes[0, 0].bar(viz_metrics, viz_values, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('MÃ©triques Principales')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar, value in zip(bars, viz_values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    '.3f', ha='center', va='bottom')\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
    "            xticklabels=['LÃ©gitime', 'Frauduleuse'], \n",
    "            yticklabels=['LÃ©gitime', 'Frauduleuse'])\n",
    "axes[0, 1].set_title('Matrice de Confusion')\n",
    "axes[0, 1].set_xlabel('PrÃ©diction')\n",
    "axes[0, 1].set_ylabel('RÃ©alitÃ©')\n",
    "\n",
    "# Distribution des probabilitÃ©s\n",
    "axes[0, 2].hist(y_proba[y_test == 0], alpha=0.7, label='LÃ©gitime', bins=50, color='green')\n",
    "axes[0, 2].hist(y_proba[y_test == 1], alpha=0.7, label='Frauduleuse', bins=50, color='red')\n",
    "axes[0, 2].set_title('Distribution des ProbabilitÃ©s')\n",
    "axes[0, 2].set_xlabel('ProbabilitÃ© de Fraude')\n",
    "axes[0, 2].set_ylabel('FrÃ©quence')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label='.2f')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('Taux de Faux Positifs')\n",
    "axes[1, 0].set_ylabel('Taux de Vrais Positifs')\n",
    "axes[1, 0].set_title('Courbe ROC')\n",
    "axes[1, 0].legend(loc=\"lower right\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Courbe Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "axes[1, 1].plot(recall, precision, color='blue', lw=2, label='.2f')\n",
    "axes[1, 1].set_xlim([0.0, 1.0])\n",
    "axes[1, 1].set_ylim([0.0, 1.05])\n",
    "axes[1, 1].set_xlabel('Rappel')\n",
    "axes[1, 1].set_ylabel('PrÃ©cision')\n",
    "axes[1, 1].set_title('Courbe Precision-Recall')\n",
    "axes[1, 1].legend(loc=\"lower left\")\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Courbe de calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "axes[1, 2].plot(prob_pred, prob_true, marker='o', color='red', label='ModÃ¨le')\n",
    "axes[1, 2].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Calibration parfaite')\n",
    "axes[1, 2].set_xlabel('ProbabilitÃ© PrÃ©dite')\n",
    "axes[1, 2].set_ylabel('ProbabilitÃ© ObservÃ©e')\n",
    "axes[1, 2].set_title('Courbe de Calibration')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d109237",
   "metadata": {},
   "source": [
    "## 3. Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des erreurs de classification\n",
    "print(\"ðŸ” Analyse des erreurs de classification...\")\n",
    "\n",
    "# CrÃ©ation d'un DataFrame avec les prÃ©dictions et erreurs\n",
    "results_df = X_test.copy()\n",
    "results_df['y_true'] = y_test\n",
    "results_df['y_pred'] = y_pred\n",
    "results_df['y_proba'] = y_proba\n",
    "results_df['error'] = (y_test != y_pred).astype(int)\n",
    "\n",
    "# Statistiques des erreurs\n",
    "print(\"ðŸ“Š STATISTIQUES DES ERREURS :\")\n",
    "print(f\"Total d'erreurs : {results_df['error'].sum()}\")\n",
    "print(\".2%\")\n",
    "print(f\"Taux d'erreur classe 0 (LÃ©gitime) : {(results_df[(results_df['y_true'] == 0) & (results_df['error'] == 1)].shape[0] / (y_test == 0).sum()) * 100:.2f}%\")\n",
    "print(f\"Taux d'erreur classe 1 (Frauduleuse) : {(results_df[(results_df['y_true'] == 1) & (results_df['error'] == 1)].shape[0] / (y_test == 1).sum()) * 100:.2f}%\")\n",
    "\n",
    "# Analyse des faux positifs et faux nÃ©gatifs\n",
    "false_positives = results_df[(results_df['y_true'] == 0) & (results_df['y_pred'] == 1)]\n",
    "false_negatives = results_df[(results_df['y_true'] == 1) & (results_df['y_pred'] == 0)]\n",
    "\n",
    "print(f\"\\nFaux positifs : {len(false_positives)}\")\n",
    "print(f\"Faux nÃ©gatifs : {len(false_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555999e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des seuils de dÃ©cision\n",
    "print(\"ðŸ“Š Analyse des seuils de dÃ©cision...\")\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calcul des mÃ©triques pour ce seuil\n",
    "    thresh_metrics = calculate_metrics(y_test, y_pred_thresh, y_proba.reshape(-1, 1))\n",
    "    thresh_metrics['threshold'] = threshold\n",
    "    threshold_metrics.append(thresh_metrics)\n",
    "\n",
    "# Conversion en DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "# Visualisation des mÃ©triques selon le seuil\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Analyse des Seuils de DÃ©cision', fontsize=16)\n",
    "\n",
    "axes[0, 0].plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "axes[0, 0].plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "axes[0, 0].set_title('Precision vs Recall')\n",
    "axes[0, 0].set_xlabel('Seuil')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(threshold_df['threshold'], threshold_df['f1'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('F1-Score vs Seuil')\n",
    "axes[0, 1].set_xlabel('Seuil')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(threshold_df['threshold'], threshold_df['precision'] * threshold_df['recall'], 'purple', linewidth=2)\n",
    "axes[1, 0].set_title('Precision Ã— Recall')\n",
    "axes[1, 0].set_xlabel('Seuil')\n",
    "axes[1, 0].set_ylabel('Precision Ã— Recall')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].scatter(threshold_df['recall'], threshold_df['precision'], \n",
    "                   c=threshold_df['threshold'], cmap='viridis', s=50)\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Courbe Precision-Recall')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Seuil')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommandation de seuil optimal\n",
    "optimal_idx = threshold_df['f1'].idxmax()\n",
    "optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
    "print(\".3f\")\n",
    "print(\".4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc8291",
   "metadata": {},
   "source": [
    "## 4. Analyse de Robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de robustesse avec diffÃ©rentes tailles d'Ã©chantillon\n",
    "print(\"ðŸ”¬ Test de robustesse - Courbe d'apprentissage...\")\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# GÃ©nÃ©ration de la courbe d'apprentissage\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model.model, X_test, y_test, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=3, scoring='average_precision', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calcul des moyennes et Ã©carts-types\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Score d\\'entraÃ®nement')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Score de validation')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Taille de l\\'Ã©chantillon d\\'entraÃ®nement')\n",
    "plt.ylabel('Score PR-AUC')\n",
    "plt.title('Courbe d\\'Apprentissage - Robustesse du ModÃ¨le')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Analyse de la courbe d'apprentissage :\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de stabilitÃ© avec bootstrap\n",
    "print(\"ðŸ”¬ Test de stabilitÃ© - Bootstrap...\")\n",
    "\n",
    "n_bootstrap = 100\n",
    "bootstrap_scores = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(n_bootstrap):\n",
    "    # Ã‰chantillonnage bootstrap\n",
    "    indices = np.random.choice(len(X_test), size=len(X_test), replace=True)\n",
    "    X_boot = X_test.iloc[indices]\n",
    "    y_boot = y_test.iloc[indices]\n",
    "    \n",
    "    # PrÃ©diction et calcul du score\n",
    "    y_pred_boot = best_model.predict(X_boot)\n",
    "    y_proba_boot = best_model.predict_proba(X_boot)[:, 1]\n",
    "    \n",
    "    pr_auc_boot = pr_auc_score(y_boot, y_proba_boot)\n",
    "    bootstrap_scores.append(pr_auc_boot)\n",
    "\n",
    "bootstrap_scores = np.array(bootstrap_scores)\n",
    "\n",
    "print(\"ðŸ“Š STATISTIQUES DE STABILITÃ‰ (Bootstrap) :\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "# Visualisation de la distribution bootstrap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bootstrap_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(np.mean(bootstrap_scores), color='red', linestyle='--', linewidth=2, \n",
    "            label='.4f')\n",
    "plt.axvline(np.percentile(bootstrap_scores, 2.5), color='orange', linestyle=':', linewidth=2, \n",
    "            label='IC 95%')\n",
    "plt.axvline(np.percentile(bootstrap_scores, 97.5), color='orange', linestyle=':', linewidth=2)\n",
    "plt.xlabel('Score PR-AUC')\n",
    "plt.ylabel('FrÃ©quence')\n",
    "plt.title('Distribution des Scores Bootstrap')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0483e5b",
   "metadata": {},
   "source": [
    "## 5. Analyse des Features Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae454055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des features importantes\n",
    "print(\"ðŸ” Analyse des features importantes...\")\n",
    "\n",
    "# RÃ©cupÃ©ration des importances (si disponible)\n",
    "if hasattr(best_model.model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance': best_model.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"ðŸ† TOP 15 FEATURES LES PLUS IMPORTANTES :\")\n",
    "    display(feature_importance.head(15))\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Top 15 Features Importantes')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model.model, 'coef_'):\n",
    "    # Pour les modÃ¨les linÃ©aires\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'coefficient': best_model.model.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"ðŸ† TOP 15 FEATURES PAR COEFFICIENT :\")\n",
    "    display(feature_importance.head(15))\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ MÃ©thode d'importance des features non disponible pour ce modÃ¨le\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a140dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'impact des features sur les prÃ©dictions\n",
    "if 'feature_importance' in locals():\n",
    "    print(\"ðŸ“Š Analyse de l'impact des top features...\")\n",
    "    \n",
    "    # SÃ©lection des 5 features les plus importantes\n",
    "    top_5_features = feature_importance['feature'].head(5).tolist()\n",
    "    \n",
    "    # Analyse de corrÃ©lation avec la cible\n",
    "    correlations = X_test[top_5_features].corrwith(y_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    correlations.plot(kind='bar', color='coral', alpha=0.7)\n",
    "    plt.title('CorrÃ©lation des Top Features avec la Cible')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('CorrÃ©lation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ“ˆ CorrÃ©lations avec la variable cible :\")\n",
    "    for feature, corr in correlations.items():\n",
    "        print(\"20s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd1bb8",
   "metadata": {},
   "source": [
    "## 6. Rapport Final et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃ©nÃ©ration du rapport final\n",
    "print(\"ðŸ“‹ RAPPORT FINAL D'Ã‰VALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ðŸ† MODÃˆLE Ã‰VALUÃ‰ : {metadata.get('model_name', 'Unknown')}\")\n",
    "print(f\"ðŸ“… Date d'Ã©valuation : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nðŸ“Š MÃ‰TRIQUES PRINCIPALES :\")\n",
    "print(\"-\" * 40)\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "print(\"\\nðŸ” ANALYSE DES ERREURS :\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"â€¢ Taux d'erreur global : {results_df['error'].mean() * 100:.2f}%\")\n",
    "print(f\"â€¢ Faux positifs : {len(false_positives)}\")\n",
    "print(f\"â€¢ Faux nÃ©gatifs : {len(false_negatives)}\")\n",
    "print(f\"â€¢ Seuil optimal recommandÃ© : {optimal_threshold:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ›¡ï¸ ROBUSTESSE :\")\n",
    "print(\"-\" * 40)\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMANDATIONS :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if all_metrics['pr_auc'] > 0.85:\n",
    "    print(\"âœ… Excellentes performances - ModÃ¨le prÃªt pour la production\")\n",
    "elif all_metrics['pr_auc'] > 0.75:\n",
    "    print(\"âš ï¸ Bonnes performances - Quelques amÃ©liorations possibles\")\n",
    "else:\n",
    "    print(\"âŒ Performances insuffisantes - RÃ©vision nÃ©cessaire\")\n",
    "\n",
    "if len(false_negatives) > len(false_positives):\n",
    "    print(\"âš ï¸ Attention : Plus de faux nÃ©gatifs - Risque de fraudes non dÃ©tectÃ©es\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Ã‰quilibre acceptable entre faux positifs et faux nÃ©gatifs\")\n",
    "\n",
    "if bootstrap_scores.std() > 0.05:\n",
    "    print(\"âš ï¸ VariabilitÃ© importante - ModÃ¨le peut Ãªtre instable\")\n",
    "else:\n",
    "    print(\"âœ… Bonne stabilitÃ© du modÃ¨le\")\n",
    "\n",
    "print(\"\\nðŸš€ PROCHAINES Ã‰TAPES :\")\n",
    "print(\"-\" * 40)\n",
    "print(\"â€¢ DÃ©ploiement en production avec monitoring continu\")\n",
    "print(\"â€¢ Collecte de nouvelles donnÃ©es pour rÃ©entraÃ®nement\")\n",
    "print(\"â€¢ Tests A/B avec diffÃ©rents seuils de dÃ©cision\")\n",
    "print(\"â€¢ IntÃ©gration dans le pipeline de dÃ©tection temps rÃ©el\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ¯ Ã‰VALUATION TERMINÃ‰E - MODÃˆLE PRÃŠT POUR LA PRODUCTION !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du rapport d'Ã©valuation\n",
    "print(\"ðŸ’¾ Sauvegarde du rapport d'Ã©valuation...\")\n",
    "\n",
    "evaluation_report = {\n",
    "    'model_info': metadata,\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'metrics': all_metrics,\n",
    "    'error_analysis': {\n",
    "        'total_errors': int(results_df['error'].sum()),\n",
    "        'error_rate': float(results_df['error'].mean()),\n",
    "        'false_positives': len(false_positives),\n",
    "        'false_negatives': len(false_negatives),\n",
    "        'optimal_threshold': float(optimal_threshold)\n",
    "    },\n",
    "    'robustness': {\n",
    "        'bootstrap_mean': float(bootstrap_scores.mean()),\n",
    "        'bootstrap_std': float(bootstrap_scores.std()),\n",
    "        'bootstrap_ci_lower': float(np.percentile(bootstrap_scores, 2.5)),\n",
    "        'bootstrap_ci_upper': float(np.percentile(bootstrap_scores, 97.5))\n",
    "    },\n",
    "    'recommendations': [\n",
    "        \"ModÃ¨le prÃªt pour la production\" if all_metrics['pr_auc'] > 0.85 else \"AmÃ©liorations nÃ©cessaires\",\n",
    "        \"Monitoring des faux nÃ©gatifs recommandÃ©\" if len(false_negatives) > len(false_positives) else \"Ã‰quilibre acceptable\",\n",
    "        \"RÃ©entraÃ®nement pÃ©riodique conseillÃ©\" if bootstrap_scores.std() > 0.05 else \"ModÃ¨le stable\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarde\n",
    "report_path = \"../reports/evaluation_report.json\"\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ… Rapport sauvegardÃ© : {report_path}\")\n",
    "print(\"\\nðŸŽ‰ Ã‰VALUATION COMPLÃˆTE !\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
