{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2da51aa",
   "metadata": {},
   "source": [
    "# √âvaluation des Mod√®les de D√©tection de Fraudes\n",
    "\n",
    "Ce notebook compare et √©value les performances des diff√©rents mod√®les avec des analyses approfondies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096b818",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Charger les mod√®les entra√Æn√©s et les comparer\n",
    "- Analyser les performances d√©taill√©es par m√©triques\n",
    "- √âtudier les erreurs et les cas limites\n",
    "- √âvaluer la robustesse des mod√®les\n",
    "- G√©n√©rer des rapports de performance complets\n",
    "- Identifier les axes d'am√©lioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ajout du chemin racine au sys.path\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Imports de base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Imports ML et m√©triques\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    average_precision_score, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Imports locaux\n",
    "from src.utils.metrics import calculate_metrics, pr_auc_score\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Environnement configur√© avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854ea76",
   "metadata": {},
   "source": [
    "## 1. Chargement des Mod√®les et Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du mod√®le entra√Æn√©\n",
    "print(\"üîÑ Chargement du mod√®le et des donn√©es...\")\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_path = \"../models/trained/best_model.pkl\"\n",
    "if os.path.exists(model_path):\n",
    "    best_model = joblib.load(model_path)\n",
    "    print(\"‚úÖ Mod√®le charg√© avec succ√®s\")\n",
    "else:\n",
    "    print(\"‚ùå Mod√®le non trouv√©. Veuillez ex√©cuter le notebook d'entra√Ænement d'abord.\")\n",
    "    raise FileNotFoundError(\"Mod√®le non trouv√©\")\n",
    "\n",
    "# Chargement des m√©tadonn√©es\n",
    "metadata_path = \"../models/metadata/model_metadata.json\"\n",
    "if os.path.exists(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"‚úÖ M√©tadonn√©es charg√©es\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è M√©tadonn√©es non trouv√©es\")\n",
    "    metadata = {}\n",
    "\n",
    "# Chargement des donn√©es de test\n",
    "features_path = \"../data/processed/features_engineered.csv\"\n",
    "if os.path.exists(features_path):\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(\"‚úÖ Donn√©es de test charg√©es\")\n",
    "else:\n",
    "    print(\"‚ùå Donn√©es de test non trouv√©es\")\n",
    "    raise FileNotFoundError(\"Donn√©es de test non trouv√©es\")\n",
    "\n",
    "print(f\"üìä Donn√©es : {df.shape}\")\n",
    "print(f\"üèÜ Mod√®le charg√© : {metadata.get('model_name', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aad8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es de test\n",
    "print(\"üîÑ Pr√©paration des donn√©es de test...\")\n",
    "\n",
    "# S√©paration features/cible\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Cr√©ation d'un jeu de test repr√©sentatif\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, X_test, _, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Test set : {X_test.shape}\")\n",
    "print(f\"üìà Distribution classes test : {y_test.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# Pr√©dictions du mod√®le\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Pr√©dictions g√©n√©r√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21a309",
   "metadata": {},
   "source": [
    "## 2. Analyse des M√©triques Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94751cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des m√©triques d√©taill√©es\n",
    "print(\"üìä Analyse des m√©triques principales...\")\n",
    "\n",
    "# M√©triques de base\n",
    "metrics = calculate_metrics(y_test, y_pred, y_proba.reshape(-1, 1))\n",
    "\n",
    "# M√©triques suppl√©mentaires\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "additional_metrics = {\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "    'cohen_kappa': cohen_kappa_score(y_test, y_pred),\n",
    "    'brier_score': brier_score_loss(y_test, y_proba)\n",
    "}\n",
    "\n",
    "# Combinaison des m√©triques\n",
    "all_metrics = {**metrics, **additional_metrics}\n",
    "\n",
    "print(\"üìà M√âTRIQUES D'√âVALUATION COMPL√àTES :\")\n",
    "print(\"-\" * 50)\n",
    "for metric_name, value in all_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(\"25s\")\n",
    "    else:\n",
    "        print(\"25s\")\n",
    "\n",
    "print(\"\\nüìã RAPPORT DE CLASSIFICATION D√âTAILL√â :\")\n",
    "print(classification_report(y_test, y_pred, target_names=['L√©gitime', 'Frauduleuse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des m√©triques principales\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('M√©triques d\\'√âvaluation - Analyse D√©taill√©e', fontsize=16)\n",
    "\n",
    "# M√©triques √† visualiser\n",
    "viz_metrics = ['precision', 'recall', 'f1', 'roc_auc', 'pr_auc', 'balanced_accuracy']\n",
    "viz_values = [all_metrics[m] for m in viz_metrics]\n",
    "\n",
    "# Bar plot des m√©triques\n",
    "bars = axes[0, 0].bar(viz_metrics, viz_values, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('M√©triques Principales')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar, value in zip(bars, viz_values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    '.3f', ha='center', va='bottom')\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
    "            xticklabels=['L√©gitime', 'Frauduleuse'], \n",
    "            yticklabels=['L√©gitime', 'Frauduleuse'])\n",
    "axes[0, 1].set_title('Matrice de Confusion')\n",
    "axes[0, 1].set_xlabel('Pr√©diction')\n",
    "axes[0, 1].set_ylabel('R√©alit√©')\n",
    "\n",
    "# Distribution des probabilit√©s\n",
    "axes[0, 2].hist(y_proba[y_test == 0], alpha=0.7, label='L√©gitime', bins=50, color='green')\n",
    "axes[0, 2].hist(y_proba[y_test == 1], alpha=0.7, label='Frauduleuse', bins=50, color='red')\n",
    "axes[0, 2].set_title('Distribution des Probabilit√©s')\n",
    "axes[0, 2].set_xlabel('Probabilit√© de Fraude')\n",
    "axes[0, 2].set_ylabel('Fr√©quence')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label='.2f')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('Taux de Faux Positifs')\n",
    "axes[1, 0].set_ylabel('Taux de Vrais Positifs')\n",
    "axes[1, 0].set_title('Courbe ROC')\n",
    "axes[1, 0].legend(loc=\"lower right\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Courbe Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "axes[1, 1].plot(recall, precision, color='blue', lw=2, label='.2f')\n",
    "axes[1, 1].set_xlim([0.0, 1.0])\n",
    "axes[1, 1].set_ylim([0.0, 1.05])\n",
    "axes[1, 1].set_xlabel('Rappel')\n",
    "axes[1, 1].set_ylabel('Pr√©cision')\n",
    "axes[1, 1].set_title('Courbe Precision-Recall')\n",
    "axes[1, 1].legend(loc=\"lower left\")\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Courbe de calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "axes[1, 2].plot(prob_pred, prob_true, marker='o', color='red', label='Mod√®le')\n",
    "axes[1, 2].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Calibration parfaite')\n",
    "axes[1, 2].set_xlabel('Probabilit√© Pr√©dite')\n",
    "axes[1, 2].set_ylabel('Probabilit√© Observ√©e')\n",
    "axes[1, 2].set_title('Courbe de Calibration')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d109237",
   "metadata": {},
   "source": [
    "## 3. Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des erreurs de classification\n",
    "print(\"üîç Analyse des erreurs de classification...\")\n",
    "\n",
    "# Cr√©ation d'un DataFrame avec les pr√©dictions et erreurs\n",
    "results_df = X_test.copy()\n",
    "results_df['y_true'] = y_test\n",
    "results_df['y_pred'] = y_pred\n",
    "results_df['y_proba'] = y_proba\n",
    "results_df['error'] = (y_test != y_pred).astype(int)\n",
    "\n",
    "# Statistiques des erreurs\n",
    "print(\"üìä STATISTIQUES DES ERREURS :\")\n",
    "print(f\"Total d'erreurs : {results_df['error'].sum()}\")\n",
    "print(\".2%\")\n",
    "print(f\"Taux d'erreur classe 0 (L√©gitime) : {(results_df[(results_df['y_true'] == 0) & (results_df['error'] == 1)].shape[0] / (y_test == 0).sum()) * 100:.2f}%\")\n",
    "print(f\"Taux d'erreur classe 1 (Frauduleuse) : {(results_df[(results_df['y_true'] == 1) & (results_df['error'] == 1)].shape[0] / (y_test == 1).sum()) * 100:.2f}%\")\n",
    "\n",
    "# Analyse des faux positifs et faux n√©gatifs\n",
    "false_positives = results_df[(results_df['y_true'] == 0) & (results_df['y_pred'] == 1)]\n",
    "false_negatives = results_df[(results_df['y_true'] == 1) & (results_df['y_pred'] == 0)]\n",
    "\n",
    "print(f\"\\nFaux positifs : {len(false_positives)}\")\n",
    "print(f\"Faux n√©gatifs : {len(false_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555999e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des seuils de d√©cision\n",
    "print(\"üìä Analyse des seuils de d√©cision...\")\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calcul des m√©triques pour ce seuil\n",
    "    thresh_metrics = calculate_metrics(y_test, y_pred_thresh, y_proba.reshape(-1, 1))\n",
    "    thresh_metrics['threshold'] = threshold\n",
    "    threshold_metrics.append(thresh_metrics)\n",
    "\n",
    "# Conversion en DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "# Visualisation des m√©triques selon le seuil\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Analyse des Seuils de D√©cision', fontsize=16)\n",
    "\n",
    "axes[0, 0].plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "axes[0, 0].plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "axes[0, 0].set_title('Precision vs Recall')\n",
    "axes[0, 0].set_xlabel('Seuil')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(threshold_df['threshold'], threshold_df['f1'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('F1-Score vs Seuil')\n",
    "axes[0, 1].set_xlabel('Seuil')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(threshold_df['threshold'], threshold_df['precision'] * threshold_df['recall'], 'purple', linewidth=2)\n",
    "axes[1, 0].set_title('Precision √ó Recall')\n",
    "axes[1, 0].set_xlabel('Seuil')\n",
    "axes[1, 0].set_ylabel('Precision √ó Recall')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].scatter(threshold_df['recall'], threshold_df['precision'], \n",
    "                   c=threshold_df['threshold'], cmap='viridis', s=50)\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Courbe Precision-Recall')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Seuil')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommandation de seuil optimal\n",
    "optimal_idx = threshold_df['f1'].idxmax()\n",
    "optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
    "print(\".3f\")\n",
    "print(\".4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc8291",
   "metadata": {},
   "source": [
    "## 4. Analyse de Robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de robustesse avec diff√©rentes tailles d'√©chantillon\n",
    "print(\"üî¨ Test de robustesse - Courbe d'apprentissage...\")\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# G√©n√©ration de la courbe d'apprentissage\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model.model, X_test, y_test, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=3, scoring='average_precision', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calcul des moyennes et √©carts-types\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Score d\\'entra√Ænement')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Score de validation')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Taille de l\\'√©chantillon d\\'entra√Ænement')\n",
    "plt.ylabel('Score PR-AUC')\n",
    "plt.title('Courbe d\\'Apprentissage - Robustesse du Mod√®le')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Analyse de la courbe d'apprentissage :\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de stabilit√© avec bootstrap\n",
    "print(\"üî¨ Test de stabilit√© - Bootstrap...\")\n",
    "\n",
    "n_bootstrap = 100\n",
    "bootstrap_scores = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(n_bootstrap):\n",
    "    # √âchantillonnage bootstrap\n",
    "    indices = np.random.choice(len(X_test), size=len(X_test), replace=True)\n",
    "    X_boot = X_test.iloc[indices]\n",
    "    y_boot = y_test.iloc[indices]\n",
    "    \n",
    "    # Pr√©diction et calcul du score\n",
    "    y_pred_boot = best_model.predict(X_boot)\n",
    "    y_proba_boot = best_model.predict_proba(X_boot)[:, 1]\n",
    "    \n",
    "    pr_auc_boot = pr_auc_score(y_boot, y_proba_boot)\n",
    "    bootstrap_scores.append(pr_auc_boot)\n",
    "\n",
    "bootstrap_scores = np.array(bootstrap_scores)\n",
    "\n",
    "print(\"üìä STATISTIQUES DE STABILIT√â (Bootstrap) :\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "# Visualisation de la distribution bootstrap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bootstrap_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(np.mean(bootstrap_scores), color='red', linestyle='--', linewidth=2, \n",
    "            label='.4f')\n",
    "plt.axvline(np.percentile(bootstrap_scores, 2.5), color='orange', linestyle=':', linewidth=2, \n",
    "            label='IC 95%')\n",
    "plt.axvline(np.percentile(bootstrap_scores, 97.5), color='orange', linestyle=':', linewidth=2)\n",
    "plt.xlabel('Score PR-AUC')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.title('Distribution des Scores Bootstrap')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0483e5b",
   "metadata": {},
   "source": [
    "## 5. Analyse des Features Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae454055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des features importantes\n",
    "print(\"üîç Analyse des features importantes...\")\n",
    "\n",
    "# R√©cup√©ration des importances (si disponible)\n",
    "if hasattr(best_model.model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance': best_model.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ TOP 15 FEATURES LES PLUS IMPORTANTES :\")\n",
    "    display(feature_importance.head(15))\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Top 15 Features Importantes')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model.model, 'coef_'):\n",
    "    # Pour les mod√®les lin√©aires\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'coefficient': best_model.model.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"üèÜ TOP 15 FEATURES PAR COEFFICIENT :\")\n",
    "    display(feature_importance.head(15))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è M√©thode d'importance des features non disponible pour ce mod√®le\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a140dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'impact des features sur les pr√©dictions\n",
    "if 'feature_importance' in locals():\n",
    "    print(\"üìä Analyse de l'impact des top features...\")\n",
    "    \n",
    "    # S√©lection des 5 features les plus importantes\n",
    "    top_5_features = feature_importance['feature'].head(5).tolist()\n",
    "    \n",
    "    # Analyse de corr√©lation avec la cible\n",
    "    correlations = X_test[top_5_features].corrwith(y_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    correlations.plot(kind='bar', color='coral', alpha=0.7)\n",
    "    plt.title('Corr√©lation des Top Features avec la Cible')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Corr√©lation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Corr√©lations avec la variable cible :\")\n",
    "    for feature, corr in correlations.items():\n",
    "        print(\"20s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd1bb8",
   "metadata": {},
   "source": [
    "## 6. Rapport Final et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration du rapport final\n",
    "print(\"üìã RAPPORT FINAL D'√âVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üèÜ MOD√àLE √âVALU√â : {metadata.get('model_name', 'Unknown')}\")\n",
    "print(f\"üìÖ Date d'√©valuation : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nüìä M√âTRIQUES PRINCIPALES :\")\n",
    "print(\"-\" * 40)\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "print(\"\\nüîç ANALYSE DES ERREURS :\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ Taux d'erreur global : {results_df['error'].mean() * 100:.2f}%\")\n",
    "print(f\"‚Ä¢ Faux positifs : {len(false_positives)}\")\n",
    "print(f\"‚Ä¢ Faux n√©gatifs : {len(false_negatives)}\")\n",
    "print(f\"‚Ä¢ Seuil optimal recommand√© : {optimal_threshold:.3f}\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è ROBUSTESSE :\")\n",
    "print(\"-\" * 40)\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "print(\".4f\")\n",
    "\n",
    "print(\"\\nüí° RECOMMANDATIONS :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if all_metrics['pr_auc'] > 0.85:\n",
    "    print(\"‚úÖ Excellentes performances - Mod√®le pr√™t pour la production\")\n",
    "elif all_metrics['pr_auc'] > 0.75:\n",
    "    print(\"‚ö†Ô∏è Bonnes performances - Quelques am√©liorations possibles\")\n",
    "else:\n",
    "    print(\"‚ùå Performances insuffisantes - R√©vision n√©cessaire\")\n",
    "\n",
    "if len(false_negatives) > len(false_positives):\n",
    "    print(\"‚ö†Ô∏è Attention : Plus de faux n√©gatifs - Risque de fraudes non d√©tect√©es\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è √âquilibre acceptable entre faux positifs et faux n√©gatifs\")\n",
    "\n",
    "if bootstrap_scores.std() > 0.05:\n",
    "    print(\"‚ö†Ô∏è Variabilit√© importante - Mod√®le peut √™tre instable\")\n",
    "else:\n",
    "    print(\"‚úÖ Bonne stabilit√© du mod√®le\")\n",
    "\n",
    "print(\"\\nüöÄ PROCHAINES √âTAPES :\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚Ä¢ D√©ploiement en production avec monitoring continu\")\n",
    "print(\"‚Ä¢ Collecte de nouvelles donn√©es pour r√©entra√Ænement\")\n",
    "print(\"‚Ä¢ Tests A/B avec diff√©rents seuils de d√©cision\")\n",
    "print(\"‚Ä¢ Int√©gration dans le pipeline de d√©tection temps r√©el\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ √âVALUATION TERMIN√âE - MOD√àLE PR√äT POUR LA PRODUCTION !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du rapport d'√©valuation\n",
    "print(\"üíæ Sauvegarde du rapport d'√©valuation...\")\n",
    "\n",
    "evaluation_report = {\n",
    "    'model_info': metadata,\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'metrics': all_metrics,\n",
    "    'error_analysis': {\n",
    "        'total_errors': int(results_df['error'].sum()),\n",
    "        'error_rate': float(results_df['error'].mean()),\n",
    "        'false_positives': len(false_positives),\n",
    "        'false_negatives': len(false_negatives),\n",
    "        'optimal_threshold': float(optimal_threshold)\n",
    "    },\n",
    "    'robustness': {\n",
    "        'bootstrap_mean': float(bootstrap_scores.mean()),\n",
    "        'bootstrap_std': float(bootstrap_scores.std()),\n",
    "        'bootstrap_ci_lower': float(np.percentile(bootstrap_scores, 2.5)),\n",
    "        'bootstrap_ci_upper': float(np.percentile(bootstrap_scores, 97.5))\n",
    "    },\n",
    "    'recommendations': [\n",
    "        \"Mod√®le pr√™t pour la production\" if all_metrics['pr_auc'] > 0.85 else \"Am√©liorations n√©cessaires\",\n",
    "        \"Monitoring des faux n√©gatifs recommand√©\" if len(false_negatives) > len(false_positives) else \"√âquilibre acceptable\",\n",
    "        \"R√©entra√Ænement p√©riodique conseill√©\" if bootstrap_scores.std() > 0.05 else \"Mod√®le stable\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarde\n",
    "report_path = \"../reports/evaluation_report.json\"\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Rapport sauvegard√© : {report_path}\")\n",
    "print(\"\\nüéâ √âVALUATION COMPL√àTE !\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
